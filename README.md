<div align="center">

# Magic Canvas ğŸ“· ğŸ–¼ï¸

</div>
Website and Jupyter Notebook examples to generate images locally (CPU-first, GPU recommended)  
Repository description: Website to create an image on CPU but suggest to use GPU. âš ï¸ GPU recommended for reasonable speed.

This README explains the project, architecture, how to set it up, model download instructions, hardware guidance, usage examples and prompts, technology stack, prompt engineering tips, known limitations, and ideas for future improvements. âœ¨

---

## Table of contents ğŸ“š
- Project overview
- Architecture ğŸ—ï¸
- Hardware requirements ğŸ’»ğŸ§®
- Setup & installation âš™ï¸
  - Python environment ğŸ
  - Installing dependencies ğŸ“¥
  - Downloading models ğŸ’¾
  - Environment variables and tokens ğŸ”’
- Running the website and notebooks â–¶ï¸
- Usage examples & example prompts âœï¸
- Technology stack & model details ğŸ§°
- Prompt engineering tips & best practices ğŸ§ 
- Limitations âš ï¸
- Future improvements ğŸ› ï¸
- License & credits ğŸ“

---

## Project overview âœ¨

AI_Image_Gen is a small project that demonstrates how to run a generative image model locally and expose a simple website (and Jupyter notebooks) to create images. The repo focuses on CPU-first usability so it can run on machines without a GPU, but GPU acceleration is strongly recommended for practical speed and larger images. ğŸš€

Use cases:
- Experiment with local image generation without external APIs ğŸ›¡ï¸
- Prototype prompts and visual styles ğŸ¨
- Educational demos using Jupyter notebooks ğŸ§ª

---

## Architecture ğŸ—ï¸

High-level components:
- Jupyter Notebooks: interactive examples and experiments (primary code in notebooks) ğŸ““
- Local model loader: downloads/loads a diffusion model into memory (Hugging Face / diffusers example) ğŸ“¥
- Inference pipeline: accepts text prompts + parameters and returns generated images (PIL/PNG) ğŸ–¼ï¸
- Simple web UI: minimal website (Flask / Streamlit) to accept prompts and display results ğŸŒ

Data / flow:
1. User enters a prompt in notebook or web UI âœï¸  
2. Server/inference code tokenizes prompt and runs the diffusion pipeline ğŸ”  
3. Generated image(s) returned to the client and saved to disk (or displayed inline) ğŸ’¾

---

## Hardware requirements ğŸ’»

CPU-only (possible but slow):
- 4+ CPU cores recommended ğŸ§®
- 16GB+ RAM recommended (some models require more) ğŸ§ 
- Swap space helpful when RAM is limited ğŸ—ƒï¸

GPU (recommended) ğŸ”¥:
- NVIDIA GPU with CUDA preferred (for PyTorch/CUDA builds) ğŸ§ª
- 8GB VRAM minimum for small images (512Ã—512) âš–ï¸
- 12+ GB VRAM recommended for larger images or higher batch sizes ğŸš€
- Use recent drivers + CUDA matching your PyTorch build ğŸ§°

Notes:
- CPU generation for 512Ã—512 can take tens of seconds to minutes depending on hardware â±ï¸
- Use smaller/distilled models for low-memory environments ğŸ§©

---
## âš ï¸ A Quick, Very Honest Performance Note 

My machine has the computational power of a confused toaster trying to run a space shuttle ğŸğŸš€ â€” so images take a while, and quality/consistency will vary. If you expect blazing results on CPU, wellâ€¦ don't. I tried. The toaster lost. ğŸ˜…

<div align="center">
  <img width="526" height="583" alt="Performance meme" src="https://github.com/user-attachments/assets/321f78c9-7cb2-4750-83fe-b42dccdadcfc" />
</div>
Common models:
- Stable Diffusion v1.4 / v1.5 (latent diffusion)
- Stable Diffusion XL (higher quality)
- Other diffusion or GAN-based models supported with adapters

Model notes:
- Some models require acceptance of licenses or authentication tokens ğŸ”’
- safetensors is recommended for safer/smaller files ğŸ§¾

---

## Prompt engineering tips & best practices ğŸ§ 

- Be specific: include style, camera terms (lens/aperture), lighting, mood, and color palette ğŸ¨
- Use adjectives: "ultra-detailed", "photorealistic", "cinematic" âœ¨
- Use negative prompts to filter undesired traits: "lowres, watermark, blurry" âŒ
- Start simple and iterate â€” add details gradually â™»ï¸
- Use seeds for reproducible results: generator=torch.Generator(device).manual_seed(SEED) ğŸ”
- For portraits, include camera/lens details: "50mm, f/1.8, rim light" ğŸ“·
- Respect model and copyright policies when invoking artists' names âš–ï¸

Advanced:
- Prompt chaining: generate variations, pick the best, refine prompts ğŸ”¬
- Combine conditioning: text + image (img2img) for guided edits ğŸ–¼ï¸â¡ï¸ğŸ–¼ï¸
- Blend prompts or use weighted prompts where supported âš–ï¸

---

## Limitations âš ï¸

- CPU performance is much slower than GPU â€” expect long runtimes on CPU â±ï¸
- Memory: large models may not fit in RAM or GPU VRAM; reduce resolution/batch size ğŸ§©
- Safety: models can produce problematic content â€” add moderation in production ğŸ›¡ï¸
- Artifacts & hallucinations: common issues (odd hands, misplaced text) ğŸŒ€
- Licensing: check model pages for usage restrictions ğŸ”
- Results can vary across hardware and numeric backends ğŸŒ

---

## Future improvements ğŸ”®

Ideas to extend the project:
- Provide Docker images with CUDA support for easy GPU deployment ğŸ³
- Add fine-tuning tools (LoRA / DreamBooth) to adapt models to custom datasets ğŸ§°
- Style-transfer and model blending features (mix CLIP embeddings) ğŸ­
- Web UI enhancements: progress bars, galleries, user accounts, seed saving ğŸ–¥ï¸
- Batch generation, queues, and multi-user scheduling âš™ï¸
- Safety moderation pipeline and content filters ğŸ›¡ï¸
- Memory optimizations: xformers, sliced attention, torch.compile (where available) âš¡
- Add img2img, inpainting, and mask-based editing features âœ‚ï¸

---

## Quick checklist to get started âœ…

1. Clone repo ğŸ“¥  
2. Create virtualenv and install dependencies ğŸ  
3. Download a supported model and set HUGGINGFACE_TOKEN if required ğŸ”‘  
4. Run Jupyter and open the sample notebook to test ğŸ““  
5. Optionally run the web app (Streamlit/Flask) ğŸŒ

---

## Models ğŸ¤–

The repository exposes the following model configurations (MODELS dictionary):

- ğŸ­ tiny
  - name: OFA-Sys/small-stable-diffusion-v0
  - size: 400MB ğŸ’¾
  - speed: Fast âš¡
  - quality: Good ğŸ‘
  - resolution: 384
  - type: stable_diffusion

- ğŸ”µ small
  - name: runwayml/stable-diffusion-v1-5
  - size: 7GB ğŸ’¾
  - speed: Medium ğŸ¢
  - quality: Excellent ğŸŒŸ
  - resolution: 512
  - type: stable_diffusion

- âœ¨ dreamshaper
  - name: Lykon/DreamShaper
  - size: 5GB ğŸ’¾
  - speed: Medium ğŸ¢
  - quality: Excellent ğŸŒŸ
  - resolution: 512
  - type: stable_diffusion

- ğŸ¨ openjourney
  - name: prompthero/openjourney-v4
  - size: 4GB ğŸ’¾
  - speed: Medium ğŸ¢
  - quality: Very Good ğŸ’«
  - resolution: 512
  - type: stable_diffusion

- mini-sd
  - name: OFA-Sys/small-stable-diffusion-v0
  - size: 400MB ğŸ’¾
  - speed: Very Fast ğŸš€
  - quality: Good ğŸ‘
  - resolution: 256
  - type: stable_diffusion

- portrait
  - name: wavymulder/portraitplus
  - size: 2GB ğŸ’¾
  - speed: Fast âš¡
  - quality: Very Good ğŸ’«
  - resolution: 384
  - type: stable_diffusion
---
## Quick start ğŸš€ (Local â€” CPU)

1. Clone the repo ğŸ“¥
```bash
git clone https://github.com/Shashankkusu/AI_Image_Gen.git
cd AI_Image_Gen
```

2. Create and activate a Python virtual environment (recommended) ğŸ
```bash
# Create venv
python3 -m venv .venv

# Activate on macOS / Linux
source .venv/bin/activate

# Activate on Windows (PowerShell)
.venv\Scripts\Activate.ps1

# Or on Windows (Command Prompt)
.venv\Scripts\activate.bat
```

3. Install dependencies ğŸ“¦
- If the repository includes a requirements file:
```bash
pip install --upgrade pip
pip install -r requirements.txt
```
- If there is no `requirements.txt`, install commonly used libraries:
```bash
pip install --upgrade pip
pip install torch torchvision transformers diffusers accelerate pillow jupyterlab
```
â„¹ï¸ Note: `sqlite3` is typically included with the standard Python distribution; if it's missing, install it via your OS package manager (e.g., `apt`, `brew`) rather than pip.

4. Run from Jupyter Notebook ğŸ““
```bash
jupyter lab    # or: jupyter notebook
```
Then open the provided notebooks in the browser.

5. Or run the app script ğŸ–¥ï¸
```bash
python app.py
```
The web interface will start and print a local URL (e.g., http://127.0.0.1:7860 or another port). Open that URL in your browser to use the prompt-based generator.

Notes & tips:
- CPU-only runs are possible but significantly slower than GPU â€” use a GPU for faster generation when available. âš ï¸
- If the project uses a different web entrypoint (e.g., `streamlit` or `flask`), check the repo files for exact commands (e.g., `streamlit run app_streamlit.py` or `FLASK_APP=app.py flask run`). ğŸ”
- If models require authentication (Hugging Face tokens), set the required environment variables before running the app.
---
Using the MODELS dictionary ğŸ—‚ï¸
- The MODELS mapping in the code contains keys (tiny, small, dreamshaper, openjourney, mini-sd, portrait).
- â• To add a model: update the MODELS dict with a new key and required attributes (`name`, `size`, `speed`, `quality`, `resolution`, `type`).
- Example:
  MODELS["my-model"] = {
    "name": "organization/model-name",
    "size": "3GB",
    "speed": "Medium",
    "quality": "Very Good",
    "resolution": 512,
    "type": "stable_diffusion"
  }
---
## Database: ai_images (SQLite) ğŸ—„ï¸

All generated images ğŸ–¼ï¸, prompts âœï¸, generation times â°, and model download metadata are stored in an SQLite file named ai_images. The DB contains three tables:

1) generated_images ğŸ“Š
-Stores each generated image metadata (filename or blob), prompt, model used, and timestamp.

ğŸ“‹ Table Structure:
<img width="1457" height="138" alt="image" src="https://github.com/user-attachments/assets/fd26bea0-8b95-4e05-8c45-9bc78688cb14" />


2) model_downloads
- Tracks downloads (or loads) of model files, useful for debugging and telemetry.

ğŸ“‹ Table Structure:
<img width="493" height="139" alt="image" src="https://github.com/user-attachments/assets/8f1a87ce-7226-4ae5-aa09-882579f5edf5" />


3) sqllite_sequence
- Standard SQLite sequence table used for AUTOINCREMENT bookkeeping (created automatically by SQLite).

ğŸ“‹ Table Structure:
<img width="211" height="113" alt="image" src="https://github.com/user-attachments/assets/3f72a077-1b8d-413f-98c8-8ccff9cc6162" />
---
## Example Usage Flow ğŸ”„

1. **User selects a model** (e.g., "small") and a resolution and enters a prompt âœï¸
2. **The backend enqueues or directly runs** the generation with the corresponding model parameters âš™ï¸
3. **When generation completes**, the resulting image is saved to disk (e.g. `./outputs/`) and metadata is inserted into `generated_images` ğŸ’¾
4. **The UI shows the generated image** and provides options to download or regenerate ğŸ“±

---
## Research Areas Related to This Project ğŸ”¬

- **GAN** (Generative Adversarial Networks) ğŸ­
- **ViT** (Vision Transformers) ğŸ‘ï¸
- **Transformers** (as general sequence models and attention mechanisms) ğŸ§ 
- **Stable Diffusion** (denoising diffusion probabilistic models used for image generation) ğŸŒªï¸
---
## Troubleshooting Tips ğŸ› ï¸

- **"Out of memory" errors**: Reduce resolution, switch to a smaller model (tiny/mini-sd), or use a machine with more RAM/GPU memory ğŸš«ğŸ’¾
- **Slow CPU generation**: Reduce steps or batch size, or run on GPU ğŸ¢
- **Missing dependencies**: Double-check the Python environment and installed packages ğŸ“¦
---

## License & credits ğŸ“

- Check the `LICENSE` file in this repository for project license ğŸ“œ  
- Models and pre-trained weights often have separate licenses â€” see the model provider page ğŸ”—  
- Credits: Hugging Face diffusers, PyTorch community, model publishers ğŸ™
- 
---
## Contact ğŸ“§

- **Repo owner**: Shashankkusu [gowrisesharoa@gmail.com] ğŸ‘¨â€ğŸ’»

